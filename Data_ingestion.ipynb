{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.functions.{col, expr, udf, _}\n",
      "import org.apache.spark.sql.{SparkSession, _}\n",
      "load: (multiline: String, delimiter: String, header: String, encoding: String, srcPath: String)org.apache.spark.sql.DataFrame\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{ col, expr, udf, _ }\n",
    "import org.apache.spark.sql.{ SparkSession, _ }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load: (multiline: String, delimiter: String, header: String, encoding: String, srcPath: String)org.apache.spark.sql.DataFrame\n",
      "srcPath: String = s3a://telenor-se-dataplatform-prod-sthlm-dswbexport/test/IN/ingestion_date=2021-04-10/*\n",
      "InputDf: org.apache.spark.sql.DataFrame = [VendorID: string, tpep_pickup_datetime: string ... 16 more fields]\n"
     ]
    }
   ],
   "source": [
    "//load data\n",
    "def load(multiline: String, delimiter: String, header: String, encoding: String, srcPath: String): DataFrame = {\n",
    "    spark.read\n",
    "      .option(\"multiLine\", multiline)\n",
    "      .option(\"delimiter\", delimiter)\n",
    "      .option(\"header\", header)\n",
    "      .option(\"escape\", \"\\\"\")\n",
    "      .option(\"encoding\", encoding)\n",
    "      .csv(srcPath)\n",
    "}\n",
    "\n",
    "val srcPath = \"s3a://telenor-se-dataplatform-prod-sthlm-dswbexport/test/IN/ingestion_date=2021-04-10/*\"\n",
    "val InputDf = load(\"false\", \",\", \"true\", \"utf-8\", srcPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getTimestampformat: (col: String, timestampFormat: String)String\n",
      "convertTypes: (df: org.apache.spark.sql.DataFrame, conversions: String, timestampFormat: String)org.apache.spark.sql.DataFrame\n",
      "conv: String = tpep_pickup_datetime->timestamp#tpep_dropoff_datetime->timestamp#passenger_count->double#trip_distance->double#fare_amount->double#mta_tax->double#tip_amount->double#tolls_amount->double#improvement_surcharge->double#total_amount->double#congestion_surcharge->double\n",
      "tsFormat: String = tpep_pickup_datetime->yyyy-MM-dd HH:mm:ss#tpep_dropoff_datetime->yyyy-MM-dd HH:mm:ss\n",
      "withDataTypesDf: org.apache.spark.sql.DataFrame = [VendorID: string, tpep_pickup_datetime: timestamp ... 16 more fields]\n"
     ]
    }
   ],
   "source": [
    "// structure the data with right schema  \n",
    "def getTimestampformat(col: String, timestampFormat: String) = {\n",
    "\n",
    "    if (!timestampFormat.isEmpty && timestampFormat.contains(\"#\")) {\n",
    "      val pairs = timestampFormat.split(\"#\").map(t => t.split(\"->\")).map {\n",
    "        case Array(k, v) => k -> v\n",
    "      }.toMap\n",
    "      pairs(col)\n",
    "    } else if (!timestampFormat.isEmpty) {\n",
    "      timestampFormat.split(\"->\")(1)\n",
    "    } else timestampFormat\n",
    "  }  \n",
    "\n",
    "def convertTypes(df: DataFrame, conversions: String, timestampFormat: String): DataFrame = {\n",
    "    if (!conversions.isEmpty) {\n",
    "      conversions.toLowerCase.split(\"#\").map(t => t.split(\"->\")).foldLeft(df) {\n",
    "        (cdf, toType) =>\n",
    "          val c = toType(0).trim()\n",
    "          val ctype = toType(1).trim()\n",
    "          if (ctype.toLowerCase() == \"timestamp\") {\n",
    "            val ts = unix_timestamp(col(c), getTimestampformat(c, timestampFormat).toString).cast(\"timestamp\")\n",
    "            cdf.withColumn(c, ts)\n",
    "          } else {\n",
    "            cdf.withColumn(c, col(c).cast(ctype))\n",
    "          }\n",
    "      }\n",
    "    } else {\n",
    "      df\n",
    "    }\n",
    "  }\n",
    "\n",
    "val conv = \"tpep_pickup_datetime->timestamp#tpep_dropoff_datetime->timestamp#passenger_count->double#trip_distance->double#fare_amount->double#mta_tax->double#tip_amount->double#tolls_amount->double#improvement_surcharge->double#total_amount->double#congestion_surcharge->double\"\n",
    "val tsFormat = \"tpep_pickup_datetime->yyyy-MM-dd HH:mm:ss#tpep_dropoff_datetime->yyyy-MM-dd HH:mm:ss\"\n",
    "val withDataTypesDf = convertTypes(InputDf, conv, tsFormat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createPartitionsIngestionDate: (srcPath: String)String\n",
      "addPartitionColumnToDataframe: (df: org.apache.spark.sql.DataFrame, srcPath: String, parrtitionColumnSql: String, partitionColumnName: String)org.apache.spark.sql.DataFrame\n",
      "parrtitionColumnSql: String = date_format(cast(tpep_pickup_datetime as timestamp),'yyyy-MM')\n",
      "partitionColumnName: String = ds\n",
      "withPartitionColDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [VendorID: string, tpep_pickup_datetime: timestamp ... 17 more fields]\n"
     ]
    }
   ],
   "source": [
    "// create partition column\n",
    "def createPartitionsIngestionDate(srcPath: String): String = {\n",
    "    val filePath = srcPath\n",
    "    val partitionValue = \"ingestion_date=[0-9]{4}-[0-9]{2}-[0-9]{2}\".r.findFirstMatchIn(filePath).getOrElse(\"error-in-pathname\").toString\n",
    "    if (partitionValue.contains(\"=\")) partitionValue.split(\"=\")(1) else partitionValue\n",
    "  }\n",
    "\n",
    "def addPartitionColumnToDataframe(df: DataFrame, srcPath: String, parrtitionColumnSql: String, partitionColumnName: String): DataFrame = {\n",
    "    if (parrtitionColumnSql.toLowerCase() == \"ingestion_date\") {\n",
    "      df.withColumn(\n",
    "        partitionColumnName,\n",
    "        lit(createPartitionsIngestionDate(srcPath)))\n",
    "    } else {\n",
    "      df.withColumn(\n",
    "        partitionColumnName,\n",
    "        expr(parrtitionColumnSql))\n",
    "    }\n",
    "  }\n",
    "\n",
    "val parrtitionColumnSql = \"date_format(cast(tpep_pickup_datetime as timestamp),'yyyy-MM')\"\n",
    "val partitionColumnName = \"ds\"\n",
    "val withPartitionColDf = addPartitionColumnToDataframe(withDataTypesDf, srcPath, parrtitionColumnSql, partitionColumnName).orderBy(col(partitionColumnName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createHiveTableFromDataframe: (df: org.apache.spark.sql.DataFrame, partitionColumnName: String, fullyQualifiedDestinationTableName: String, destinationPath: String)Unit\n",
      "CREATE EXTERNAL TABLE IF NOT EXISTS  default.nyc (\n",
      "       VendorID  string,\n",
      "tpep_pickup_datetime  timestamp,\n",
      "tpep_dropoff_datetime  timestamp,\n",
      "passenger_count  double,\n",
      "trip_distance  double,\n",
      "RatecodeID  string,\n",
      "store_and_fwd_flag  string,\n",
      "PULocationID  string,\n",
      "DOLocationID  string,\n",
      "payment_type  string,\n",
      "fare_amount  double,\n",
      "extra  string,\n",
      "mta_tax  double,\n",
      "tip_amount  double,\n",
      "tolls_amount  double,\n",
      "improvement_surcharge  double,\n",
      "total_amount  double,\n",
      "congestion_surcharge  double )\n",
      "       PARTITIONED BY ( ds string)\n",
      "       STORED AS PARQUET\n",
      "       LOCATION 's3a://telenor-se-dataplatform-prod-sthlm-dswbexport/test/OUT/NYC/'\n"
     ]
    }
   ],
   "source": [
    "// create hive table if not exists \n",
    "\n",
    "def createHiveTableFromDataframe(df: DataFrame, partitionColumnName: String, fullyQualifiedDestinationTableName: String, destinationPath: String) {\n",
    "    val fields = df.schema.fields.filterNot(\n",
    "      _.name\n",
    "        .trim()\n",
    "        .equalsIgnoreCase(partitionColumnName)) //:+ partitionField\n",
    "    val bdy =\n",
    "      fields.map(f => s\"${f.name}  ${f.dataType.catalogString}\").mkString(\",\\n\")\n",
    "\n",
    "    val stmt =\n",
    "      s\"\"\"\n",
    "      CREATE EXTERNAL TABLE IF NOT EXISTS  ${fullyQualifiedDestinationTableName} (\n",
    "       $bdy )\n",
    "       PARTITIONED BY ( ${partitionColumnName} string)\n",
    "       STORED AS PARQUET\n",
    "       LOCATION '${destinationPath}'\n",
    "      \"\"\"\n",
    "    println(stmt.trim())\n",
    "    spark.sql(stmt)\n",
    "  }\n",
    "\n",
    "createHiveTableFromDataframe(withPartitionColDf, \"ds\", \"default.nyc\", \"s3a://telenor-se-dataplatform-prod-sthlm-dswbexport/test/OUT/NYC/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writeParquetFile: (df: org.apache.spark.sql.DataFrame, fullyQualifiedDestinationTableName: String, partitionColumnName: String)org.apache.spark.sql.DataFrame\n",
      "res106: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "// write the parquet file\n",
    "\n",
    "def writeParquetFile(df: DataFrame, fullyQualifiedDestinationTableName: String, partitionColumnName: String) = {\n",
    "    val tmpTbl =  \"tmp_tbl\"\n",
    "    df.createOrReplaceTempView(tmpTbl)\n",
    "\n",
    "    val insertStmt =\n",
    "      s\"\"\"INSERT OVERWRITE TABLE ${fullyQualifiedDestinationTableName}\n",
    "        PARTITION(${partitionColumnName})\n",
    "        SELECT *\n",
    "        FROM $tmpTbl\n",
    "        \"\"\".stripMargin\n",
    "    spark.sql(insertStmt)\n",
    "  }\n",
    "\n",
    "\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\", true)\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "writeParquetFile(withPartitionColDf, \"default.nyc\", \"ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|     ds|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-------+\n",
      "|       2| 2020-02-02 13:14:41|  2020-02-02 13:21:03|            2.0|         0.85|         1|                 N|          48|         246|           2|        6.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         9.8|                 2.5|2020-02|\n",
      "|       2| 2020-02-13 20:51:11|  2020-02-13 21:21:25|            1.0|        13.29|         2|                 N|          10|         237|           1|       52.0|    0|    0.5|     11.06|         0.0|                  0.3|       66.36|                 2.5|2020-02|\n",
      "|       2| 2020-02-02 13:27:19|  2020-02-02 13:31:02|            3.0|         1.17|         1|                 N|          50|         142|           2|        5.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         9.3|                 2.5|2020-02|\n",
      "|       1| 2020-02-13 20:30:11|  2020-02-13 20:35:23|            1.0|          1.0|         1|                 N|         143|         238|           1|        6.0|    3|    0.5|      1.95|         0.0|                  0.3|       11.75|                 2.5|2020-02|\n",
      "|       2| 2020-02-23 21:26:23|  2020-02-23 21:44:39|            3.0|         6.29|         1|                 N|         230|          88|           1|       21.0|  0.5|    0.5|       4.0|         0.0|                  0.3|        28.8|                 2.5|2020-02|\n",
      "|       1| 2020-02-13 20:37:44|  2020-02-13 20:40:47|            1.0|          0.7|         1|                 N|         238|         151|           1|        4.5|    3|    0.5|       1.0|         0.0|                  0.3|         9.3|                 2.5|2020-02|\n",
      "|       1| 2020-02-01 00:17:35|  2020-02-01 00:30:32|            1.0|          2.6|         1|                 N|         145|           7|           1|       11.0|  0.5|    0.5|      2.45|         0.0|                  0.3|       14.75|                 0.0|2020-02|\n",
      "|       2| 2020-02-13 20:39:08|  2020-02-13 20:46:26|            1.0|         1.51|         1|                 N|         249|         231|           1|        7.5|  0.5|    0.5|       1.0|         0.0|                  0.3|        12.3|                 2.5|2020-02|\n",
      "|       1| 2020-02-01 00:32:47|  2020-02-01 01:05:36|            1.0|          4.8|         1|                 N|          45|          61|           1|       21.5|    3|    0.5|       6.3|         0.0|                  0.3|        31.6|                 2.5|2020-02|\n",
      "|       2| 2020-02-13 20:52:57|  2020-02-13 21:11:13|            1.0|         4.64|         1|                 N|         125|         163|           1|       17.0|  0.5|    0.5|      4.16|         0.0|                  0.3|       24.96|                 2.5|2020-02|\n",
      "|       1| 2020-02-01 00:31:44|  2020-02-01 00:43:28|            1.0|          3.2|         1|                 N|         186|         140|           1|       11.0|    3|    0.5|       1.0|         0.0|                  0.3|        15.8|                 2.5|2020-02|\n",
      "|       2| 2020-02-13 20:23:12|  2020-02-13 20:36:53|            1.0|         1.33|         1|                 N|         229|         100|           1|       10.0|  0.5|    0.5|       1.0|         0.0|                  0.3|        14.8|                 2.5|2020-02|\n",
      "|       2| 2020-02-01 00:07:35|  2020-02-01 00:31:39|            1.0|         4.38|         1|                 N|         144|         140|           1|       18.0|  0.5|    0.5|       3.0|         0.0|                  0.3|        24.8|                 2.5|2020-02|\n",
      "|       2| 2020-02-13 20:41:33|  2020-02-13 20:51:31|            1.0|         0.98|         1|                 N|          68|          48|           1|        8.0|  0.5|    0.5|      2.36|         0.0|                  0.3|       14.16|                 2.5|2020-02|\n",
      "|       2| 2020-02-01 00:51:43|  2020-02-01 01:01:29|            1.0|         2.28|         1|                 N|         238|         152|           2|        9.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        10.8|                 0.0|2020-02|\n",
      "|       2| 2020-02-13 20:58:07|  2020-02-13 21:07:09|            1.0|          1.7|         1|                 N|         230|         234|           1|        8.0|  0.5|    0.5|      2.36|         0.0|                  0.3|       14.16|                 2.5|2020-02|\n",
      "|       1| 2020-02-01 00:15:49|  2020-02-01 00:20:48|            2.0|          1.0|         1|                 N|         249|         107|           1|        5.5|    3|    0.5|      1.85|         0.0|                  0.3|       11.15|                 2.5|2020-02|\n",
      "|       1| 2020-02-13 20:29:08|  2020-02-13 20:44:16|            2.0|          2.3|         1|                 N|         113|         229|           1|       11.5|    3|    0.5|      3.06|         0.0|                  0.3|       18.36|                 2.5|2020-02|\n",
      "|       1| 2020-02-01 00:25:31|  2020-02-01 00:50:22|            2.0|          3.4|         1|                 N|          79|         256|           1|       18.5|    3|    0.5|      4.45|         0.0|                  0.3|       26.75|                 2.5|2020-02|\n",
      "|       1| 2020-02-13 20:45:08|  2020-02-13 21:08:03|            1.0|          8.1|         1|                 N|         229|         231|           1|       26.5|    3|    0.5|       2.5|         0.0|                  0.3|        32.8|                 2.5|2020-02|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from default.nyc\").show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
